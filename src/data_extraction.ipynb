{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import sqlite3\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "URL = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n",
    "DB_PATH = \"data/highest_grossing_films.db\"\n",
    "JSON_PATH = \"data/films.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wikipedia_page(url: str) -> str:\n",
    "    \"\"\"Fetch HTML content from Wikipedia\"\"\"\n",
    "    print(f\"Fetching page from {url}\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers, timeout=30)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Successfully fetched page\")\n",
    "        return response.text\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_content(html_content: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract film data from HTML content\"\"\"\n",
    "    print(\"Parsing HTML content...\")\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "        print(\"Using lxml parser\")\n",
    "    except Exception:\n",
    "        print(\"Falling back to html.parser\")\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Create data directory and save debug HTML\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    with open('data/debug_page.html', 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    # Method 1: Try to find tables with 'wikitable' class\n",
    "    print(\"Looking for tables with class 'wikitable'\")\n",
    "    tables = soup.find_all('table', class_='wikitable')\n",
    "    print(f\"Found {len(tables)} tables with class 'wikitable'\")\n",
    "    \n",
    "    if not tables:\n",
    "        # Method 2: Try to find any table\n",
    "        print(\"Looking for any tables\")\n",
    "        tables = soup.find_all('table')\n",
    "        print(f\"Found {len(tables)} tables\")\n",
    "        \n",
    "        if not tables:\n",
    "            print(\"No tables found in the page\")\n",
    "            return []\n",
    "    \n",
    "    # Method 3: Look for headings related to highest-grossing films\n",
    "    section_heading = soup.find(lambda tag: tag.name in ['h1', 'h2', 'h3'] and \n",
    "                               'highest-grossing films' in tag.text.lower())\n",
    "    if section_heading:\n",
    "        print(f\"Found relevant section: {section_heading.text}\")\n",
    "        # Try to find the table after this heading\n",
    "        section_table = section_heading.find_next('table')\n",
    "        if section_table:\n",
    "            print(\"Found table after the section heading\")\n",
    "            tables.insert(0, section_table)  # Prioritize this table\n",
    "    \n",
    "    # Find the table with relevant headers\n",
    "    print(\"Looking for table with film data...\")\n",
    "    table = None\n",
    "    for i, t in enumerate(tables):\n",
    "        headers = [h.text.strip().lower() for h in t.find_all('th')]\n",
    "        print(f\"Table {i+1} headers: {headers}\")\n",
    "        \n",
    "        # Try different conditions to find the right table\n",
    "        if any(h for h in headers if 'title' in h or 'film' in h):\n",
    "            if any(h for h in headers if 'box office' in h or 'gross' in h or 'worldwide' in h):\n",
    "                table = t\n",
    "                print(f\"Found relevant table at index {i+1}\")\n",
    "                break\n",
    "            \n",
    "    # Fallback - just use the first table if we couldn't identify one by headers\n",
    "    if not table and tables:\n",
    "        table = tables[0]\n",
    "        print(\"Using first table as fallback\")\n",
    "    \n",
    "    if not table:\n",
    "        print(\"No suitable table found\")\n",
    "        return []\n",
    "    \n",
    "    # Extract column indices\n",
    "    header_row = table.find('tr')\n",
    "    headers = [th.text.strip().lower() for th in header_row.find_all('th')]\n",
    "    print(f\"Header row: {headers}\")\n",
    "    \n",
    "    # Find column indices with fallbacks\n",
    "    title_idx = -1\n",
    "    for i, h in enumerate(headers):\n",
    "        if 'title' in h or 'film' in h:\n",
    "            title_idx = i\n",
    "            break\n",
    "    if title_idx == -1:\n",
    "        title_idx = 1 if len(headers) > 1 else 0\n",
    "        \n",
    "    year_idx = -1\n",
    "    for i, h in enumerate(headers):\n",
    "        if 'year' in h or 'released' in h or 'release' in h:\n",
    "            year_idx = i\n",
    "            break\n",
    "            \n",
    "    box_office_idx = -1\n",
    "    for i, h in enumerate(headers):\n",
    "        if 'box office' in h or 'gross' in h or 'worldwide' in h:\n",
    "            box_office_idx = i\n",
    "            break\n",
    "    if box_office_idx == -1:\n",
    "        # Try to find a column with currency symbols\n",
    "        for i, h in enumerate(headers):\n",
    "            if '$' in h or '¥' in h or '€' in h:\n",
    "                box_office_idx = i\n",
    "                break\n",
    "        if box_office_idx == -1:\n",
    "            box_office_idx = 2 if len(headers) > 2 else len(headers) - 1\n",
    "    \n",
    "    print(f\"Using indices - Title: {title_idx}, Year: {year_idx}, Box Office: {box_office_idx}\")\n",
    "    \n",
    "    # Extract data from rows\n",
    "    films_data = []\n",
    "    rows = table.find_all('tr')[1:]  # Skip header row\n",
    "    print(f\"Processing {len(rows)} data rows\")\n",
    "    \n",
    "    for row in rows:\n",
    "        cells = row.find_all(['th', 'td'])\n",
    "        \n",
    "        if len(cells) <= max(title_idx, box_office_idx):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Extract title and wiki link\n",
    "            title_cell = cells[title_idx]\n",
    "            title_link = title_cell.find('a')\n",
    "            \n",
    "            if title_link:\n",
    "                title = title_link.text.strip()\n",
    "                wiki_link = title_link.get('href', '')\n",
    "            else:\n",
    "                title = title_cell.text.strip()\n",
    "                wiki_link = \"\"\n",
    "            \n",
    "            # Clean title (remove footnotes)\n",
    "            title = re.sub(r'\\[\\d+\\]', '', title).strip()\n",
    "            \n",
    "            # Extract box office value\n",
    "            box_office_text = cells[box_office_idx].text.strip()\n",
    "            box_office_text = re.sub(r'\\[\\d+\\]', '', box_office_text)\n",
    "            box_office_value = re.sub(r'[^\\d.]', '', box_office_text)\n",
    "            \n",
    "            try:\n",
    "                box_office = float(box_office_value) if box_office_value else 0.0\n",
    "                # Fix extremely large values\n",
    "                if box_office > 5000000000:\n",
    "                    magnitude = len(str(int(box_office)))\n",
    "                    if magnitude > 10:\n",
    "                        box_office = box_office / (10 ** (magnitude - 10))\n",
    "            except ValueError:\n",
    "                box_office = 0.0\n",
    "            \n",
    "            # Extract year\n",
    "            year = 0\n",
    "            if year_idx != -1 and year_idx < len(cells):\n",
    "                year_text = cells[year_idx].text.strip()\n",
    "                year_match = re.search(r'\\b(19\\d{2}|20\\d{2})\\b', year_text)\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(1))\n",
    "            else:\n",
    "                # Try to extract year from title\n",
    "                year_match = re.search(r'\\((\\d{4})\\)', title)\n",
    "                if year_match:\n",
    "                    year = int(year_match.group(1))\n",
    "                    title = re.sub(r'\\s*\\(\\d{4}\\)', '', title).strip()\n",
    "            \n",
    "            # Create film data\n",
    "            film_data = {\n",
    "                'title': title,\n",
    "                'release_year': year,\n",
    "                'box_office': box_office,\n",
    "                'wiki_link': wiki_link,\n",
    "                'director': \"Unknown\",\n",
    "                'country': \"Unknown\",\n",
    "            }\n",
    "            \n",
    "            films_data.append(film_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Extracted data for {len(films_data)} films\")\n",
    "    \n",
    "    # Show a sample of extracted data\n",
    "    if films_data:\n",
    "        print(\"Sample of extracted data:\")\n",
    "        for i, film in enumerate(films_data[:3]):\n",
    "            print(f\"  {i+1}. {film['title']} - Year: {film['release_year']}, Box Office: {film['box_office']}\")\n",
    "    \n",
    "    return films_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_film_details(film_url: str) -> Dict[str, str]:\n",
    "    \"\"\"Extract director and country from film's Wikipedia page\"\"\"\n",
    "    details = {\"director\": \"Unknown\", \"country\": \"Unknown\"}\n",
    "    \n",
    "    if not film_url:\n",
    "        return details\n",
    "    \n",
    "    # Maximum number of retries\n",
    "    max_retries = 3\n",
    "    base_timeout = 10\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Exponential backoff for timeout and retry delay\n",
    "            current_timeout = base_timeout * (2 ** attempt)\n",
    "            \n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124'}\n",
    "            full_url = f\"https://en.wikipedia.org{film_url}\"\n",
    "            print(f\"Fetching details from: {full_url} (Attempt {attempt+1}/{max_retries})\")\n",
    "            \n",
    "            response = requests.get(full_url, headers=headers, timeout=current_timeout)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Received status code {response.status_code}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 2 ** attempt\n",
    "                    print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                continue\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            info_box = soup.find('table', class_='infobox')\n",
    "            \n",
    "            if info_box:\n",
    "                # Extract director\n",
    "                director_rows = info_box.find_all(lambda tag: tag.name == 'th' and \n",
    "                                                ('Directed by' in tag.text or 'Director' in tag.text))\n",
    "                \n",
    "                for row in director_rows:\n",
    "                    if row and row.find_next('td'):\n",
    "                        director_text = row.find_next('td').text.strip()\n",
    "                        # Clean and get the first director\n",
    "                        director_text = re.sub(r'\\[\\d+\\]', '', director_text)  # Remove citations\n",
    "                        # Split by common separators and take the first one\n",
    "                        director_parts = re.split(r',|\\band\\b|;|\\|', director_text, maxsplit=1)\n",
    "                        details[\"director\"] = director_parts[0].strip()\n",
    "                        break\n",
    "                \n",
    "                # Extract country\n",
    "                country_rows = info_box.find_all(lambda tag: tag.name == 'th' and \n",
    "                                               ('Country' in tag.text or 'Countries' in tag.text))\n",
    "                \n",
    "                for row in country_rows:\n",
    "                    if row and row.find_next('td'):\n",
    "                        country_text = row.find_next('td').text.strip()\n",
    "                        # Clean and get the first country\n",
    "                        country_text = re.sub(r'\\[\\d+\\]', '', country_text)  # Remove citations\n",
    "                        # Split by common separators and take the first one\n",
    "                        country_parts = re.split(r',|\\band\\b|;|\\|', country_text, maxsplit=1)\n",
    "                        details[\"country\"] = country_parts[0].strip()\n",
    "                        break\n",
    "            \n",
    "            # Clean up extracted text\n",
    "            for key in details:\n",
    "                if details[key] != \"Unknown\":\n",
    "                    details[key] = re.sub(r'\\s+', ' ', details[key]).strip()  # Clean whitespace\n",
    "            \n",
    "            # If we got here without exception, break the retry loop\n",
    "            break\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout occurred (Attempt {attempt+1}/{max_retries})\")\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt\n",
    "                print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching details: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt\n",
    "                print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "    \n",
    "    print(f\"Extracted details: Director={details['director']}, Country={details['country']}\")\n",
    "    return details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_film_data(films_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Add director and country information to film data\"\"\"\n",
    "    print(\"Enriching film data with directors and countries...\")\n",
    "    enriched_data = [dict(film) for film in films_data]\n",
    "    \n",
    "    # Track consecutive failures for adaptive delay\n",
    "    consecutive_failures = 0\n",
    "    base_delay = 1.0\n",
    "    \n",
    "    for i, film in enumerate(enriched_data):\n",
    "        if film['wiki_link']:\n",
    "            print(f\"Processing film {i+1}/{len(enriched_data)}: {film['title']}\")\n",
    "            \n",
    "            # Adaptive delay based on consecutive failures\n",
    "            current_delay = base_delay\n",
    "            if consecutive_failures > 0:\n",
    "                # Increase delay if we've had failures (up to 10 seconds)\n",
    "                current_delay = min(base_delay * (2 ** consecutive_failures), 10.0)\n",
    "            \n",
    "            # Add randomization to delay (±30%)\n",
    "            jitter = current_delay * 0.3 * (random.random() * 2 - 1)\n",
    "            actual_delay = max(0.5, current_delay + jitter)\n",
    "            \n",
    "            print(f\"Waiting {actual_delay:.2f} seconds before next request...\")\n",
    "            time.sleep(actual_delay)\n",
    "            \n",
    "            # Try to get details\n",
    "            details = get_film_details(film['wiki_link'])\n",
    "            \n",
    "            # Track failures/successes to adjust delay dynamically\n",
    "            if details['director'] == \"Unknown\" and details['country'] == \"Unknown\":\n",
    "                consecutive_failures += 1\n",
    "                print(f\"Extraction failed. Consecutive failures: {consecutive_failures}\")\n",
    "            else:\n",
    "                # Reset counter after a successful extraction\n",
    "                if consecutive_failures > 0:\n",
    "                    consecutive_failures = max(0, consecutive_failures - 1)\n",
    "            \n",
    "            film['director'] = details['director']\n",
    "            film['country'] = details['country']\n",
    "    \n",
    "    return enriched_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(films_data: List[Dict[str, Any]]) -> None:\n",
    "    \"\"\"Store film data in SQLite database\"\"\"\n",
    "    print(\"Creating SQLite database...\")\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create table\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS films (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        title TEXT NOT NULL,\n",
    "        release_year INTEGER,\n",
    "        director TEXT,\n",
    "        box_office REAL,\n",
    "        country TEXT\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    # Clear existing data\n",
    "    cursor.execute(\"DELETE FROM films\")\n",
    "    \n",
    "    # Insert data\n",
    "    for film in films_data:\n",
    "        cursor.execute('''\n",
    "        INSERT INTO films (title, release_year, director, box_office, country)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            film['title'],\n",
    "            film['release_year'],\n",
    "            film['director'],\n",
    "            film['box_office'],\n",
    "            film['country']\n",
    "        ))\n",
    "    \n",
    "    conn.commit()\n",
    "    \n",
    "    # Verify data was inserted\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM films\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"Inserted {count} records into the database\")\n",
    "    \n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_json() -> None:\n",
    "    \"\"\"Export database data to JSON file\"\"\"\n",
    "    print(\"Exporting data to JSON...\")\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    conn.row_factory = sqlite3.Row\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(\"SELECT * FROM films\")\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    films_json = []\n",
    "    for row in rows:\n",
    "        film_dict = {}\n",
    "        for key in row.keys():\n",
    "            film_dict[key] = row[key]\n",
    "        films_json.append(film_dict)\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    with open(JSON_PATH, 'w') as json_file:\n",
    "        json.dump(films_json, json_file, indent=4)\n",
    "    \n",
    "    print(f\"Exported {len(films_json)} films to JSON file: {JSON_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page from https://en.wikipedia.org/wiki/List_of_highest-grossing_films\n",
      "Successfully fetched page\n",
      "Parsing HTML content...\n",
      "Falling back to html.parser\n",
      "Looking for tables with class 'wikitable'\n",
      "Found 85 tables with class 'wikitable'\n",
      "Found relevant section: List of highest-grossing films\n",
      "Found table after the section heading\n",
      "Looking for table with film data...\n",
      "Table 1 headers: ['rank', 'peak', 'title', 'worldwide gross', 'year', 'ref', 'avatar', 'avengers: endgame', 'avatar: the way of water', 'titanic', 'star wars: the force awakens', 'avengers: infinity war', 'ne zha 2 †', 'spider-man: no way home', 'inside out 2', 'jurassic world', 'the lion king', 'the avengers', 'furious 7', 'top gun: maverick', 'frozen 2', 'barbie', 'avengers: age of ultron', 'the super mario bros. movie', 'black panther', 'harry potter and the deathly hallows – part 2', 'deadpool & wolverine', 'star wars: the last jedi', 'jurassic world: fallen kingdom', 'frozen', 'beauty and the beast', 'incredibles 2', 'the fate of the furious', 'iron man 3', 'minions', 'captain america: civil war', 'aquaman', 'the lord of the rings: the return of the king', 'spider-man: far from home', 'captain marvel', 'transformers: dark of the moon', 'skyfall', 'transformers: age of extinction', 'the dark knight rises', 'joker', 'star wars: the rise of skywalker', 'toy story 4', 'toy story 3', \"pirates of the caribbean: dead man's chest\", 'rogue one: a star wars story', 'moana 2 †', 'aladdin', 'star wars: episode i – the phantom menace', 'pirates of the caribbean: on stranger tides', 'jurassic park', 'despicable me 3']\n",
      "Found relevant table at index 1\n",
      "Header row: ['rank', 'peak', 'title', 'worldwide gross', 'year', 'ref']\n",
      "Using indices - Title: 2, Year: 4, Box Office: 3\n",
      "Processing 50 data rows\n",
      "Extracted data for 50 films\n",
      "Sample of extracted data:\n",
      "  1. Avatar - Year: 2009, Box Office: 2923706026.0\n",
      "  2. Avengers: Endgame - Year: 2019, Box Office: 2797501328.0\n",
      "  3. Avatar: The Way of Water - Year: 2022, Box Office: 2320250281.0\n",
      "Enriching film data with directors and countries...\n",
      "Processing film 1/50: Avatar\n",
      "Waiting 1.03 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Avatar_(2009_film) (Attempt 1/3)\n",
      "Extracted details: Director=James Cameron, Country=United Kingdom United States\n",
      "Processing film 2/50: Avengers: Endgame\n",
      "Waiting 0.88 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Avengers:_Endgame (Attempt 1/3)\n",
      "Extracted details: Director=Anthony RussoJoe Russo, Country=United States\n",
      "Processing film 3/50: Avatar: The Way of Water\n",
      "Waiting 1.09 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Avatar:_The_Way_of_Water (Attempt 1/3)\n",
      "Extracted details: Director=James Cameron, Country=United States\n",
      "Processing film 4/50: Titanic\n",
      "Waiting 0.97 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Titanic_(1997_film) (Attempt 1/3)\n",
      "Extracted details: Director=James Cameron, Country=United States\n",
      "Processing film 5/50: Star Wars: The Force Awakens\n",
      "Waiting 0.79 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Star_Wars:_The_Force_Awakens (Attempt 1/3)\n",
      "Extracted details: Director=J. J. Abrams, Country=United States\n",
      "Processing film 6/50: Avengers: Infinity War\n",
      "Waiting 0.72 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Avengers:_Infinity_War (Attempt 1/3)\n",
      "Extracted details: Director=Anthony RussoJoe Russo, Country=United States\n",
      "Processing film 7/50: Ne Zha 2\n",
      "Waiting 0.73 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Ne_Zha_2 (Attempt 1/3)\n",
      "Extracted details: Director=Jiaozi, Country=China\n",
      "Processing film 8/50: Spider-Man: No Way Home\n",
      "Waiting 1.29 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Spider-Man:_No_Way_Home (Attempt 1/3)\n",
      "Extracted details: Director=Jon Watts, Country=United States\n",
      "Processing film 9/50: Inside Out 2\n",
      "Waiting 0.71 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Inside_Out_2 (Attempt 1/3)\n",
      "Extracted details: Director=Kelsey Mann, Country=United States\n",
      "Processing film 10/50: Jurassic World\n",
      "Waiting 0.89 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Jurassic_World (Attempt 1/3)\n",
      "Extracted details: Director=Colin Trevorrow, Country=United States\n",
      "Processing film 11/50: The Lion King\n",
      "Waiting 0.98 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/The_Lion_King_(2019_film) (Attempt 1/3)\n",
      "Extracted details: Director=Jon Favreau, Country=United States\n",
      "Processing film 12/50: The Avengers\n",
      "Waiting 1.09 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/The_Avengers_(2012_film) (Attempt 1/3)\n",
      "Extracted details: Director=Joss Whedon, Country=United States\n",
      "Processing film 13/50: Furious 7\n",
      "Waiting 1.25 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Furious_7 (Attempt 1/3)\n",
      "Timeout occurred (Attempt 1/3)\n",
      "Retrying in 1 seconds...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Furious_7 (Attempt 2/3)\n",
      "Extracted details: Director=James Wan, Country=United States China\n",
      "Processing film 14/50: Top Gun: Maverick\n",
      "Waiting 0.77 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Top_Gun:_Maverick (Attempt 1/3)\n",
      "Extracted details: Director=Joseph Kosinski, Country=United States\n",
      "Processing film 15/50: Frozen 2\n",
      "Waiting 1.16 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Frozen_2 (Attempt 1/3)\n",
      "Extracted details: Director=Chris Buck Jennifer Lee, Country=United States\n",
      "Processing film 16/50: Barbie\n",
      "Waiting 1.18 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Barbie_(film) (Attempt 1/3)\n",
      "Extracted details: Director=Greta Gerwig, Country=United States United Kingdom\n",
      "Processing film 17/50: Avengers: Age of Ultron\n",
      "Waiting 0.99 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Avengers:_Age_of_Ultron (Attempt 1/3)\n",
      "Extracted details: Director=Joss Whedon, Country=United States\n",
      "Processing film 18/50: The Super Mario Bros. Movie\n",
      "Waiting 1.24 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/The_Super_Mario_Bros._Movie (Attempt 1/3)\n",
      "Extracted details: Director=Aaron Horvath Michael Jelenic, Country=United States\n",
      "Processing film 19/50: Black Panther\n",
      "Waiting 1.16 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Black_Panther_(film) (Attempt 1/3)\n",
      "Extracted details: Director=Ryan Coogler, Country=United States\n",
      "Processing film 20/50: Harry Potter and the Deathly Hallows – Part 2\n",
      "Waiting 1.02 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Harry_Potter_and_the_Deathly_Hallows_%E2%80%93_Part_2 (Attempt 1/3)\n",
      "Extracted details: Director=David Yates, Country=United Kingdom United States\n",
      "Processing film 21/50: Deadpool & Wolverine\n",
      "Waiting 1.09 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Deadpool_%26_Wolverine (Attempt 1/3)\n",
      "Extracted details: Director=Shawn Levy, Country=United States\n",
      "Processing film 22/50: Star Wars: The Last Jedi\n",
      "Waiting 0.82 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Star_Wars:_The_Last_Jedi (Attempt 1/3)\n",
      "Extracted details: Director=Rian Johnson, Country=United States\n",
      "Processing film 23/50: Jurassic World: Fallen Kingdom\n",
      "Waiting 1.15 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Jurassic_World:_Fallen_Kingdom (Attempt 1/3)\n",
      "Extracted details: Director=J. A. Bayona, Country=China United States\n",
      "Processing film 24/50: Frozen\n",
      "Waiting 1.21 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Frozen_(2013_film) (Attempt 1/3)\n",
      "Extracted details: Director=Chris Buck Jennifer Lee, Country=United States\n",
      "Processing film 25/50: Beauty and the Beast\n",
      "Waiting 0.77 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Beauty_and_the_Beast_(2017_film) (Attempt 1/3)\n",
      "Extracted details: Director=Bill Condon, Country=United States\n",
      "Processing film 26/50: Incredibles 2\n",
      "Waiting 0.93 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Incredibles_2 (Attempt 1/3)\n",
      "Extracted details: Director=Brad Bird, Country=United States\n",
      "Processing film 27/50: The Fate of the Furious\n",
      "Waiting 0.87 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/The_Fate_of_the_Furious (Attempt 1/3)\n",
      "Extracted details: Director=F. Gary Gray, Country=United States China\n",
      "Processing film 28/50: Iron Man 3\n",
      "Waiting 0.80 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Iron_Man_3 (Attempt 1/3)\n",
      "Extracted details: Director=Shane Black, Country=United States\n",
      "Processing film 29/50: Minions\n",
      "Waiting 0.77 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Minions_(film) (Attempt 1/3)\n",
      "Extracted details: Director=Pierre Coffin Kyle Balda, Country=United States\n",
      "Processing film 30/50: Captain America: Civil War\n",
      "Waiting 1.29 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Captain_America:_Civil_War (Attempt 1/3)\n",
      "Extracted details: Director=Anthony RussoJoe Russo, Country=United States\n",
      "Processing film 31/50: Aquaman\n",
      "Waiting 1.29 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Aquaman_(film) (Attempt 1/3)\n",
      "Extracted details: Director=James Wan, Country=United States\n",
      "Processing film 32/50: The Lord of the Rings: The Return of the King\n",
      "Waiting 0.80 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/The_Lord_of_the_Rings:_The_Return_of_the_King (Attempt 1/3)\n",
      "Extracted details: Director=Peter Jackson, Country=New Zealand Germany United States\n",
      "Processing film 33/50: Spider-Man: Far From Home\n",
      "Waiting 1.26 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Spider-Man:_Far_From_Home (Attempt 1/3)\n",
      "Extracted details: Director=Jon Watts, Country=United States\n",
      "Processing film 34/50: Captain Marvel\n",
      "Waiting 1.02 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Captain_Marvel_(film) (Attempt 1/3)\n",
      "Extracted details: Director=Anna BodenRyan Fleck, Country=United States\n",
      "Processing film 35/50: Transformers: Dark of the Moon\n",
      "Waiting 1.19 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Transformers:_Dark_of_the_Moon (Attempt 1/3)\n",
      "Timeout occurred (Attempt 1/3)\n",
      "Retrying in 1 seconds...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Transformers:_Dark_of_the_Moon (Attempt 2/3)\n",
      "Extracted details: Director=Michael Bay, Country=United States\n",
      "Processing film 36/50: Skyfall\n",
      "Waiting 0.78 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Skyfall (Attempt 1/3)\n",
      "Extracted details: Director=Sam Mendes, Country=United KingdomUnited States\n",
      "Processing film 37/50: Transformers: Age of Extinction\n",
      "Waiting 0.87 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Transformers:_Age_of_Extinction (Attempt 1/3)\n",
      "Extracted details: Director=Michael Bay, Country=United States\n",
      "Processing film 38/50: The Dark Knight Rises\n",
      "Waiting 0.85 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/The_Dark_Knight_Rises (Attempt 1/3)\n",
      "Extracted details: Director=Christopher Nolan, Country=United States United Kingdom\n",
      "Processing film 39/50: Joker\n",
      "Waiting 1.04 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Joker_(2019_film) (Attempt 1/3)\n",
      "Extracted details: Director=Todd Phillips, Country=United States\n",
      "Processing film 40/50: Star Wars: The Rise of Skywalker\n",
      "Waiting 0.94 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Star_Wars:_The_Rise_of_Skywalker (Attempt 1/3)\n",
      "Extracted details: Director=J. J. Abrams, Country=United States\n",
      "Processing film 41/50: Toy Story 4\n",
      "Waiting 0.99 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Toy_Story_4 (Attempt 1/3)\n",
      "Extracted details: Director=Josh Cooley, Country=United States\n",
      "Processing film 42/50: Toy Story 3\n",
      "Waiting 1.13 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Toy_Story_3 (Attempt 1/3)\n",
      "Extracted details: Director=Lee Unkrich, Country=United States\n",
      "Processing film 43/50: Pirates of the Caribbean: Dead Man's Chest\n",
      "Waiting 0.82 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Pirates_of_the_Caribbean:_Dead_Man%27s_Chest (Attempt 1/3)\n",
      "Extracted details: Director=Gore Verbinski, Country=United States\n",
      "Processing film 44/50: Rogue One: A Star Wars Story\n",
      "Waiting 0.89 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Rogue_One:_A_Star_Wars_Story (Attempt 1/3)\n",
      "Extracted details: Director=Gareth Edwards, Country=United States\n",
      "Processing film 45/50: Moana 2\n",
      "Waiting 1.26 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Moana_2 (Attempt 1/3)\n",
      "Extracted details: Director=David Derrick Jr. Jason Hand Dana Ledoux Miller, Country=United States\n",
      "Processing film 46/50: Aladdin\n",
      "Waiting 0.95 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Aladdin_(2019_film) (Attempt 1/3)\n",
      "Extracted details: Director=Guy Ritchie, Country=United States\n",
      "Processing film 47/50: Star Wars: Episode I – The Phantom Menace\n",
      "Waiting 0.74 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Star_Wars:_Episode_I_%E2%80%93_The_Phantom_Menace (Attempt 1/3)\n",
      "Extracted details: Director=George Lucas, Country=United States\n",
      "Processing film 48/50: Pirates of the Caribbean: On Stranger Tides\n",
      "Waiting 1.11 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Pirates_of_the_Caribbean:_On_Stranger_Tides (Attempt 1/3)\n",
      "Extracted details: Director=Rob Marshall, Country=United States\n",
      "Processing film 49/50: Jurassic Park\n",
      "Waiting 1.26 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Jurassic_Park_(film) (Attempt 1/3)\n",
      "Extracted details: Director=Steven Spielberg, Country=United States\n",
      "Processing film 50/50: Despicable Me 3\n",
      "Waiting 0.95 seconds before next request...\n",
      "Fetching details from: https://en.wikipedia.org/wiki/Despicable_Me_3 (Attempt 1/3)\n",
      "Extracted details: Director=Pierre Coffin Kyle Balda, Country=United States\n",
      "Creating SQLite database...\n",
      "Inserted 50 records into the database\n",
      "Exporting data to JSON...\n",
      "Exported 50 films to JSON file: data/films.json\n",
      "Data extraction completed. Processed 50 films.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Fetch and parse Wikipedia data\n",
    "html_content = fetch_wikipedia_page(URL)\n",
    "films_data = parse_html_content(html_content)\n",
    "\n",
    "# Step 2: Enrich with director and country info\n",
    "enriched_data = enrich_film_data(films_data)\n",
    "# Step 3: Store in database\n",
    "create_database(enriched_data)\n",
    "\n",
    "# Step 4: Export to JSON\n",
    "export_to_json()\n",
    "\n",
    "print(f\"Data extraction completed. Processed {len(enriched_data)} films.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
